diff --git a/src/diffusers/models/attention_processor.py b/src/diffusers/models/attention_processor.py
index 21eb3a32..1df1b09c 100644
--- a/src/diffusers/models/attention_processor.py
+++ b/src/diffusers/models/attention_processor.py
@@ -200,10 +200,8 @@ class Attention(nn.Module):
         # We use the AttnProcessor2_0 by default when torch 2.x is used which uses
         # torch.nn.functional.scaled_dot_product_attention for native Flash/memory_efficient_attention
         # but only if it has the default `scale` argument. TODO remove scale_qk check when we move to torch 2.1
-        if processor is None:
-            processor = (
-                AttnProcessor2_0() if hasattr(F, "scaled_dot_product_attention") and self.scale_qk else AttnProcessor()
-            )
+        # force to not use FlashAttention
+        processor = AttnProcessor()
         self.set_processor(processor)

     def set_use_memory_efficient_attention_xformers(
@@ -588,7 +586,9 @@ class Attention(nn.Module):

         if attention_mask is None:
             baddbmm_input = torch.empty(
-                query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device
+                query.shape[0], query.shape[1],
+                key.shape[2], # key is already transposed
+                dtype=query.dtype, device=query.device
             )
             beta = 0
         else:
@@ -598,7 +598,7 @@ class Attention(nn.Module):
         attention_scores = torch.baddbmm(
             baddbmm_input,
             query,
-            key.transpose(-1, -2),
+            key,  # key is already transposed
             beta=beta,
             alpha=self.scale,
         )
@@ -740,8 +740,26 @@ class AttnProcessor:
         key = attn.head_to_batch_dim(key)
         value = attn.head_to_batch_dim(value)

-        attention_probs = attn.get_attention_scores(query, key, attention_mask)
-        hidden_states = torch.bmm(attention_probs, value)
+        # pre-transpose the key
+        key = key.transpose(-1, -2)
+        if query.size(-2) != value.size(-2): # cross-attention, use regular attention
+            # QKV done in single block
+            attention_probs = attn.get_attention_scores(query, key, attention_mask)
+            hidden_states = torch.bmm(attention_probs, value)
+        else: # self-attention, use blocked attention
+            # QKV done with block-attention (a la FlashAttentionV2)
+            print(f"{query.shape = }, {key.shape = }, {value.shape = }")
+            query_block_size = 256
+            query_seq_len = query.size(-2)
+            num_blocks = (query_seq_len + query_block_size - 1) // query_block_size
+            for qidx in range(num_blocks):
+                query_block = query[:,qidx*query_block_size:(qidx+1)*query_block_size,:]
+                attention_probs = attn.get_attention_scores(query_block, key, attention_mask)
+                hidden_states_block = torch.bmm(attention_probs, value)
+                if qidx == 0:
+                    hidden_states = hidden_states_block
+                else:
+                    hidden_states = torch.cat((hidden_states, hidden_states_block), -2)
         hidden_states = attn.batch_to_head_dim(hidden_states)

         # linear proj
