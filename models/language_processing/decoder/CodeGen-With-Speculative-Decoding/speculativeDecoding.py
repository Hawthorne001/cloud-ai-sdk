##############################################################################
#
# Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause-Clear
#
##############################################################################

import json
import os
from time import perf_counter
from typing import Dict, List, Optional
from pdb import set_trace as bp
import numpy as np
import transformers

np.random.seed(42)
from qaic_infer import QAICInferenceSession

from rich.console import Console

console = Console()

DEBUG = 1
UNDERSCORE_SPECIAL_TOKEN = 9601
#using 3 newlines as stop sequence
CODEGEN_SINGLE_NEWLINE_ID = 198
CODEGEN_DOUBLE_NEWLINE_ID = 628
def debugprint(*args):
    global DEBUG
    if False:
        print(args)

time_dict = dict()
n_tokens_accepted_histogram = dict()
def update_time(key, start_time):
    global time_dict
    if DEBUG == 0:
        return
    if key not in time_dict.keys():
        time_dict[key] = 0
    time_dict[key] += (perf_counter() - start_time)

io_files = []

vocab_size_dict = {"llama2":32000, "codegen":51200, "starcoder":49152}

stop_sequences = [
        "\n\n\n"
      ]

def main(
    tlm_session,
    dlm_session,
    tokenizer,
    model_family: str,
    prompt_len: int,
    ctx_len: int,
    prompt: List[str],
    max_spec_length: int,
    stream: bool = True,
) -> Dict[str, float]:

    prompt=[prompt]

    # Load QPC
    assert (model_family in vocab_size_dict.keys()), f"unsupported model family {model_family}"
    vocab_size = vocab_size_dict[model_family]

    #Why do we need to do this?  Should be unneccessary...
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # Prepare inputs for first iteration
    start = perf_counter()

    #run decode on DLM
    #initially, input_ids consist of the prompt, then subsequently they become either 1-long or 2-long vectors based on MRS outcome
    #position ids starts with the first token being assigned 0, and keeps increasing by 1 for each subsequent token
    dlm_inputs = dict()
    tlm_inputs = dict()
    last_accepted_token = []
    overall_num_tokens_accepted = 0
    dlm_cache_index = np.array([0])
    tlm_cache_index = np.array([0])
    batch_size = len(prompt)
    logits_out_placeholder = np.zeros((batch_size, vocab_size), dtype=np.float16)
    if model_family == "codegen" or model_family == "starcoder":
        logits_out_placeholder = np.expand_dims(logits_out_placeholder, 1)
    #TODO handle batch_size>1 for prompt number of valid tokens
    prompt_num_valid_tokens = np.zeros((batch_size,1))
    #to store the final tokens accepted
    generated_ids = np.full((batch_size, ctx_len - prompt_len + 1), tokenizer.pad_token_id)
    num_dlm_tokens_rejected = -1
    num_iters=0
    #store the new token generated by TLM in the previous iteration
    tlm_last_iter_newtoken_id = tokenizer.pad_token_id
    is_first_iter = True
    total_dlm_time = 0
    total_dlm_devrun_time = 0
    total_tlm_time = 0
    total_tlm_devrun_time = 0
    total_dlm_iter = 0
    total_tlm_iter = 0
    curr_dlm_time = 0
    curr_tlm_time = 0
    n_ttft = 0

    tlm_logits = np.full((batch_size, max_spec_length+1, vocab_size), 0., dtype=np.float32)
    dlm_session.set_buffers({"logits":logits_out_placeholder})
    #chunking prompts start
    prompt_chunk_size = max(
        [x[dlm_session.binding_index_map["input_ids"]][1][1] for x in dlm_session.allowed_shapes]
        + [dlm_session.bindings[dlm_session.binding_index_map["input_ids"]].dims[1]]
    )
    print("Prompt Chunk Size:", prompt_chunk_size)
    prompt_len = tokenizer(prompt, return_tensors="np", padding=True).input_ids.shape[1]
    num_chunks = -(prompt_len // -prompt_chunk_size)  # ceil divide without float
    prompt_len = num_chunks * prompt_chunk_size
    print("Num Chunks: ", num_chunks)
    print("New Prompt Len: ", prompt_len)
    #chunking prompts end
    tokens_accepted_in_first_round=0
    is_tlm_prefill = True
    # print out the prompt string
    print()
    if stream:
        console.print(f"{prompt[0] if isinstance(prompt, list) else prompt}", end="", highlight=False)
    print()
    while (overall_num_tokens_accepted + prompt_len) <  ctx_len:
        if n_ttft == 1:
          tokens_accepted_in_first_round = overall_num_tokens_accepted
          start_decode_throughput_counter = perf_counter()
        #only speculate up to the position id that's < maximum model context length
        stop_generation = False
        start_dlm = perf_counter()
        spec_length = min(max_spec_length, (ctx_len - prompt_len - overall_num_tokens_accepted))
        #to store the speculated tokens from DLM
        dlm_candidate_ids = np.full((batch_size, max_spec_length), tokenizer.pad_token_id)
        #to store the speculated token logits from DLM

        dlm_infer_start = perf_counter()
        for i in range(max_spec_length):
            dlm_preproc_start = perf_counter()
            if dlm_cache_index[0] == 0:#prefill
                dlm_inputs = tokenizer(prompt, return_tensors="np", padding="max_length", max_length=prompt_len)
                dlm_session.skip_buffers(set([x for x in dlm_session.input_names if x.startswith("past_")]))
                num_tokens = prompt_len
                dlm_inputs["position_ids"] = (np.cumsum(dlm_inputs["attention_mask"], 1) - 1) * dlm_inputs["attention_mask"]
                #FIXME the below line assumes batch size =1 and hence takes a full sum instead of along axis=1
                prompt_num_valid_tokens = np.sum(dlm_inputs["attention_mask"])
                dlm_inputs["attention_mask"] = np.concatenate(
                    [
                        dlm_inputs["attention_mask"].astype(bool),
                        np.zeros((batch_size, ctx_len - prompt_len), dtype=bool),
                    ],
                    1,
                )
                dlm_inputs["cache_index"] = dlm_cache_index

            else:#decode or precode
                dlm_inputs["attention_mask"] = np.zeros_like(dlm_inputs["attention_mask"])
                #FIXME in prompt chunking case input len >= prompt len
                dlm_inputs["attention_mask"][0,prompt_len-prompt_num_valid_tokens:prompt_len+overall_num_tokens_accepted+i] = 1
                if num_dlm_tokens_rejected == 0:#run in precode mode for 2 input tokens
                    #no tokens were rejected in the previous TLM evaluation
                    # input has to be 2 tokens as well as their position_ids: (last_token_of_prev_speculation, new_token)
                    #reset the value of num_dlm_tokens_rejected to -1 to allow further single token decode speculation
                    num_dlm_tokens_rejected = -1
                    num_tokens = 2
                else:
                    num_tokens = 1
                    if len(next_token_id.shape) > 2:
                        next_token_id = np.squeeze(next_token_id, axis=-1)
                    dlm_inputs["input_ids"] = next_token_id
                    if dlm_cache_index[0] == prompt_len:
                        #previous iteration was a prefill
                        # Skip attention_mask from this iteration onwards to use retained attention_mask
                        dlm_inputs["position_ids"] = dlm_inputs["attention_mask"].sum(1, keepdims=True) - 1
                    if dlm_inputs["position_ids"].shape[1] > 1:
                        #previous iteration was a precode
                        #next position id should be 1 more than that of the tlm_new_token
                        dlm_inputs["position_ids"] = dlm_inputs["position_ids"][:,1:2] + 1
                    elif not dlm_cache_index[0] == prompt_len:
                        #previous iteration was a decode
                        dlm_inputs["position_ids"] += 1
            if dlm_cache_index[0] > prompt_len:
                #only decode/precode time counted
                update_time("dlm_preproc", dlm_preproc_start)
            dlm_devrun_start = perf_counter()
            dlm_outputs = []
            if dlm_cache_index[0] == 0: #prefill
                num_tokens=0 #prompt chunking requires cache index to be updated in chunk_size intervals per prefill iter
                for pi in range(num_chunks):
                    chunk_inputs = dlm_inputs.copy()
                    chunk_inputs["input_ids"] = dlm_inputs["input_ids"][:, dlm_cache_index[0] : dlm_cache_index[0] + prompt_chunk_size]
                    chunk_inputs["position_ids"] = dlm_inputs["position_ids"][
                        :, dlm_cache_index[0] : dlm_cache_index[0] + prompt_chunk_size
                    ]
                    chunk_inputs["attention_mask"] = dlm_inputs["attention_mask"].copy()
                    chunk_inputs["attention_mask"][:, dlm_cache_index[0] + prompt_chunk_size :] = False
                    dlm_outputs = dlm_session.run(chunk_inputs)
                    dlm_cache_index[0] += prompt_chunk_size
            else:
                dlm_outputs = dlm_session.run(dlm_inputs)
            if dlm_cache_index[0] > 0:
                #only decode/precode time counted
                update_time("dlm_run", dlm_devrun_start)
            dlm_devrun_end=perf_counter()
            total_dlm_devrun_time += dlm_devrun_end-dlm_devrun_start
            total_dlm_iter += 1
            logits_out_dlm_fp = dlm_outputs["logits"].astype(np.float32)
            # greedy sampling
            dlm_greedy_stime=perf_counter()
            next_token_id = np.expand_dims(np.argmax(logits_out_dlm_fp,axis=-1),0)
            update_time("dlm_sample", dlm_greedy_stime)
            dlm_cache_index += num_tokens
            dlm_candidate_ids[:, i] = next_token_id
        dlm_infer_end = perf_counter()
        curr_dlm_time = dlm_infer_end - dlm_infer_start
        total_dlm_time = total_dlm_time + curr_dlm_time

        tlm_infer_start = perf_counter()
        # in both cases, we want to extract the final spec_length + 1 logits
        # (1st iter prefill +)oneshot decode pass over the TLM 
        tlm_prefill_logit_out = None
        num_tlm_input_tokens = max_spec_length + 1 #for non-prefill scenario, we always provide the prev iteration's TLM token a followed by the speculation

        if is_tlm_prefill == True:
            tlm_prefill_stime = perf_counter()

            #tokenize the prompt, padding it to the prompt_len
            tlm_inputs = tokenizer(prompt, return_tensors="np", padding="max_length", max_length=prompt_len)
            # TODO fix for batch_size > 1, fix for when context-length-limit is reached
            #  when speculation can be smaller than spec_length(pad to the tlm_prompt_len)
            tlm_inputs["position_ids"] = (np.cumsum(tlm_inputs["attention_mask"], 1) - 1) * tlm_inputs["attention_mask"]
            tlm_session.skip_buffers(set([x for x in tlm_session.input_names if x.startswith("past_")]))
            tlm_inputs["attention_mask"] = np.concatenate(
                [
                    tlm_inputs["attention_mask"].astype(bool),
                    np.zeros((batch_size, ctx_len - prompt_len), dtype=bool),
                ],
                1,
            )
            num_tokens = prompt_len
            tlm_inputs["cache_index"] = tlm_cache_index # = 0 for prefill
            #run prefill on TLM and get the logits and last token from here (used to evaluate the first speculated token)
            tlm_session.set_buffers({"logits":np.zeros((batch_size, prompt_chunk_size, vocab_size), dtype=np.float16)})
            tlm_devrun_start = perf_counter()
            for pi in range(num_chunks):
                chunk_inputs = tlm_inputs.copy()
                chunk_inputs["input_ids"] = tlm_inputs["input_ids"][:, tlm_cache_index[0] : tlm_cache_index[0] + prompt_chunk_size]
                chunk_inputs["position_ids"] = tlm_inputs["position_ids"][
                    :, tlm_cache_index[0] : tlm_cache_index[0] + prompt_chunk_size
                ]
                chunk_inputs["attention_mask"] = tlm_inputs["attention_mask"].copy()
                chunk_inputs["attention_mask"][:, tlm_cache_index[0] + prompt_chunk_size :] = False
                tlm_prefill_outputs = tlm_session.run(chunk_inputs)
                tlm_cache_index[0] += prompt_chunk_size
            tlm_devrun_end = perf_counter()
            total_tlm_devrun_time+=tlm_devrun_end-tlm_devrun_start
            total_tlm_iter+=1

            logits_out_tlm_prefill = tlm_prefill_outputs["logits"]
            if len(logits_out_tlm_prefill.shape) == 2:
                logits_out_tlm_prefill = np.expand_dims(logits_out_tlm_prefill, 1)
            #first token speculated will be evaluated using the logits output at the last token position of the prefill
            tlm_prefill_logit_out = logits_out_tlm_prefill[:,-1:]

            tlm_prefill_etime = perf_counter()

        #PRECODE tlm evaluation + new_token
        # check if this is the first precode pass
        tlm_precode_stime = perf_counter()
        tlm_inputs["cache_index"] = tlm_cache_index
        tlm_position_id_start = None
        if tlm_cache_index[0] == prompt_len and is_first_iter:
            is_first_iter=False
            #just performed a PREFILL earlier in this iteration
            #get position ids by summing the attention mask
            num_tlm_input_tokens = max_spec_length #in the case that we just performed a prefill, we will provide only the current iter speculation as the input tokens
            tlm_position_id_start = tlm_inputs["attention_mask"].sum(1, keepdims=True)
            tlm_inputs["input_ids"] = dlm_candidate_ids
            tlm_inputs["attention_mask"][0,prompt_len:prompt_len+overall_num_tokens_accepted+num_tlm_input_tokens-0] = 1
        else:
            #prefill was not done in this spd iter, so we need to provide the prev iter's TLM-token as input to get it's KV$
            #TODO fix for batch_size>1
            tlm_position_id_start = prompt_num_valid_tokens + overall_num_tokens_accepted - 1

            tlm_inputs["input_ids"] = np.concatenate((tlm_last_iter_newtoken_id, dlm_candidate_ids),axis=1)
            tlm_inputs["attention_mask"][0,prompt_len:prompt_len+overall_num_tokens_accepted+num_tlm_input_tokens-1] = 1
        #TODO fix for batch_size > 1
        tlm_inputs["position_ids"] = np.expand_dims(np.arange(tlm_position_id_start,tlm_position_id_start+num_tlm_input_tokens),axis=0)
        tlm_session.set_buffers({"logits":np.zeros((batch_size, num_tlm_input_tokens, vocab_size), dtype=np.float16)})
        update_time("tlm_preproc",tlm_precode_stime)

        tlm_devrun_start = perf_counter()
        tlm_precode_outputs = tlm_session.run(tlm_inputs)
        update_time("tlm_run", tlm_devrun_start)
        total_tlm_iter+=1
        #Can modify this if TLM outputs perfect shapes TBD:
        logits_out_tlm_precode = tlm_precode_outputs["logits"][:,:num_tlm_input_tokens]#does this do it?
        tlm_cache_index += num_tlm_input_tokens
        # retrieve logits for the first spec_length + 1 positions (since we pad to the right here)
        if is_tlm_prefill:
            #the first spec_length-1 logits of this current PRECODE are the q(x2) till q(x_spec_length)
            # q(x1) was obtained as the output of the PREFILL stage that happened earlier in this same iteration
            # the last logit is for the new_token_if_all_prev_accepted
            tlm_logits = np.concatenate((tlm_prefill_logit_out,logits_out_tlm_precode[:,:spec_length]), axis=1)
        else:
            # the first spec_length logits are the q(x1) till q(x_spec_length)
            # the last token is the q(new_token_if_all_prev_accepted)
            tlm_logits = logits_out_tlm_precode[:,:spec_length+1]
        # last position is to be used for sampling the new token
        # the previous spec_length to be used for MRS on the speculated logits

        candidate_sel_stime = perf_counter()
        #TODO fix for batch_size > 1
        select_upto = 0
        for spec_idx in range(spec_length):
            debugprint(f"MRS loop at speculation index : {spec_idx}")
            correct_tlm_sample_token = tokenizer.pad_token_id
            draft_token = np.expand_dims(dlm_candidate_ids[:,spec_idx],1)
            is_accepted=False
            # greedy sampling:
            greedy_tlm_stime=perf_counter()
            tlm_token = np.argmax(np.expand_dims(tlm_logits[:,spec_idx],1), axis=-1)
            update_time("tlm_sample", greedy_tlm_stime)
            correct_tlm_sample_token = tlm_token
            exact_match_stime=perf_counter()
            #exact matching
            is_accepted = tlm_token == draft_token

            update_time("exact_match", exact_match_stime)
            if is_accepted:
                select_upto += 1
            else:
                new_tlm_id = correct_tlm_sample_token
                break
        if DEBUG == 1:
            if select_upto not in n_tokens_accepted_histogram.keys():
                n_tokens_accepted_histogram[select_upto] = 0
            n_tokens_accepted_histogram[select_upto] += 1
        debugprint("Number of tokens accepted this MRS round:", select_upto)
        debugprint("prompt_len + overall_num_tokens_accepted=",(prompt_len+overall_num_tokens_accepted))
        # add selected candidate ids to the generated ids
        #TODO fix for batch_size > 1
        output_update_stime=perf_counter()
        if select_upto > 0:
            generated_ids[0,overall_num_tokens_accepted:overall_num_tokens_accepted+select_upto] = dlm_candidate_ids[0,:select_upto]
        if n_ttft == 0:
            first_token_time = perf_counter()
            ttft  = first_token_time - start

        n_ttft = n_ttft + 1
        #if all speculated tokens were accepted, append the new TLM-generated id here
        if select_upto == spec_length:
            if dlm_candidate_ids[0,spec_length-1] == tokenizer.eos_token_id or spec_length<max_spec_length:
                #stop here, dont sample new token from TLM since we have either generated an eos or are at the end of ctx_len
                pass
            else: #greedy
                new_tlm_id = np.argmax(tlm_logits[:,select_upto:select_upto+1],axis=-1)
        this_iter_gen_ids = list(generated_ids[0, overall_num_tokens_accepted:overall_num_tokens_accepted+select_upto]) + list(new_tlm_id[0])

        if stream:
            color = "white"
            if model_family == "llama2":
                nextlinecount = 0
                #use convert ids to tokens here to find and utilize the prefix-spaces to decode correct sentences
                curr_token_list = tokenizer.convert_ids_to_tokens(this_iter_gen_ids, skip_special_tokens=True)
                for i,tok in enumerate(curr_token_list):
                    if this_iter_gen_ids[i] == tokenizer.eos_token_id:
                        stop_generation=True
                        break
                    if i == (len(curr_token_list)-1):
                        color="white"
                    else:
                        color="deep_sky_blue1"
                    curr_token = tok
                    curr_token = curr_token.replace("<0x0A>","\n")
                    if curr_token == "\n":
                        nextlinecount += 1
                        if nextlinecount >= 3:
                            stop_generation = True
                            break
                    else:
                        nextlinecount = 0
                    if ord(curr_token[0]) == UNDERSCORE_SPECIAL_TOKEN:
                        console.print(f" {curr_token[1:]}",end="", highlight=False)

                    else:
                        console.print(f"{curr_token}",end="", highlight=False)

            elif model_family == "codegen" or model_family == "starcoder":
                #check for triple newlines
                check_ids = last_accepted_token + this_iter_gen_ids
                #check for the stop sequence in these check_tokens
                stop_seq_idx = -1
                for it in range(len(check_ids)-1):
                    if this_iter_gen_ids[it] == tokenizer.eos_token_id:
                        stop_seq_idx = it
                        break
                    if check_ids[it] == CODEGEN_SINGLE_NEWLINE_ID:
                        if check_ids[it+1] == CODEGEN_DOUBLE_NEWLINE_ID:
                                stop_seq_idx = it
                                break
                    elif check_ids[it] == CODEGEN_DOUBLE_NEWLINE_ID:
                        if check_ids[it+1] == CODEGEN_SINGLE_NEWLINE_ID:
                                stop_seq_idx = it
                                break
                #stream output
                for i,thisiterid in enumerate(this_iter_gen_ids):
                    if i == stop_seq_idx:
                        stop_generation = True
                        break
                    if i == (len(this_iter_gen_ids)-1):
                        color="white"
                    else:
                        color="deep_sky_blue1"
                    console.print(f"[{color}]{tokenizer.decode(thisiterid)}", end="", highlight=False)
                #update the last accepted token
                last_accepted_token = check_ids[-1:]
        overall_num_tokens_accepted += select_upto
        if new_tlm_id is not None:
            generated_ids[0,overall_num_tokens_accepted] = new_tlm_id
            next_token_id = new_tlm_id
            tlm_last_iter_newtoken_id = new_tlm_id
            overall_num_tokens_accepted += 1

        if stop_generation:
            break
        num_dlm_tokens_rejected = spec_length - select_upto

        # update input ids, position ids and cache indices for DLM and TLM for the next pass
        if num_dlm_tokens_rejected > 0:
            #DLM cache rewind
            assert(dlm_inputs["position_ids"].shape[1] == 1)
            dlm_cache_index -= (num_dlm_tokens_rejected - 1)
            dlm_inputs["position_ids"] -= (num_dlm_tokens_rejected - 1) #since the input in next iter is the new tlm token from this iter
            ##TLM cache rewind
            tlm_cache_index -= (num_dlm_tokens_rejected)

        else:
            #DLM precode to be run: if all tokens were accepted, then we need to generate KV$ for the last speculated token
            # TODO fix for batch_size > 1
            dlm_inputs["input_ids"] = np.concatenate((dlm_candidate_ids[:,-1:],new_tlm_id), axis=1)
            dlm_inputs["position_ids"] = np.concatenate((dlm_inputs["position_ids"]+1, dlm_inputs["position_ids"]+2), axis=1)
            
        candidate_sel_etime = perf_counter()
        update_time("output_cache_update", output_update_stime)
        num_iters+=1
        tlm_infer_end = perf_counter()
        curr_tlm_time = tlm_infer_end - tlm_infer_start
        total_tlm_time = total_tlm_time + curr_tlm_time 
        is_tlm_prefill = False
    end = perf_counter()
    tlm_overhead = total_tlm_time - total_tlm_devrun_time
    tlm_time_per_iter = total_tlm_time / total_tlm_iter
    tlm_iter_per_time = 1 / tlm_time_per_iter
    tlm_time_per_iter = total_tlm_devrun_time / total_tlm_iter
    tlm_iter_per_time = 1 / tlm_time_per_iter
    dlm_overhead = total_dlm_time - total_dlm_devrun_time
    dlm_time_per_iter = total_dlm_time / total_dlm_iter
    dlm_iter_per_time = 1 / dlm_time_per_iter
    dlm_time_per_iter = total_dlm_devrun_time / total_dlm_iter
    dlm_iter_per_time = 1 / dlm_time_per_iter
    total_perf = (overall_num_tokens_accepted) / (end - start)
    decode_only_perf = (overall_num_tokens_accepted - tokens_accepted_in_first_round) / (end - start_decode_throughput_counter)

    debugprint()
    print()
    print()
    print()
    print("="*60)
    print("Token Generation Rate:\t", round(decode_only_perf, 2), "tokens per second")
    #subtract the free token when calculating acceptance rate
    acceptance_rate = (overall_num_tokens_accepted/total_tlm_iter) - 1
    print("Acceptance Rate:\t", round(acceptance_rate, 2))
    print("="*60)
    print()
    print()
    if DEBUG == 1:
        total_time=0
        for k, v in time_dict.items():
            print(f"{k},{v}")
            total_time+=v
        print(f"time to first token,{ttft}")
        print(f"end - start,{end-start}")
        print(f"total time,{total_time}")
        print(f"overall number of generated tokens,{overall_num_tokens_accepted}")
        num_tok_drafted = overall_num_tokens_accepted* max_spec_length / (1+acceptance_rate)
        dlm_tok_rate = num_tok_drafted / time_dict["dlm_run"]
        print(f"number of tokens drafted,{num_tok_drafted}")
        print(f"draft model token rate,{dlm_tok_rate}")
        num_tok_tlm = overall_num_tokens_accepted * 1. / (1 + acceptance_rate)
        tlm_tok_rate = num_tok_tlm / time_dict["tlm_run"]
        print(f"number of tokens from target model,{num_tok_tlm}")
        print(f"target model token rate,{tlm_tok_rate}")
        print("\n\n acceptance histogram:")
        print("n_accepted,n_times")
        for k,v in n_tokens_accepted_histogram.items():
            print(f"{k},{v}")

if __name__ == "__main__":
    import argparse

    argp = argparse.ArgumentParser()
    argp.add_argument("--model-family", required=True, help="Model family, choose from \"llama2\" or \"codegen\"")
    argp.add_argument("--model-name", required=True, help="Model name to run")
    argp.add_argument("--prompt-len", type=int, default=32, help="Prompt length")
    argp.add_argument("--ctx-len", type=int, default=128, help="Context length")
    argp.add_argument("--dlm-qpc", required=True, help="Compiled binary DLM QPC")
    argp.add_argument("--tlm-qpc", required=True, help="Compiled binary TLM QPC")
    argp.add_argument(
        "--prompt",
        default="My name is",
        help="Input prompt(s) to generate for (pipe-separated)",
    )
    argp.add_argument(
        "--no-stream", action="store_false", dest="stream", help="Don't stream output text"
    )
    argp.add_argument(
        "--dlm-device_id",
        default=[0],
        type=lambda device_ids: [int(x) for x in device_ids.split(",")],
        help="QAIC device ids to run DLM on (comma-separated)",
    )
    argp.add_argument(
        "--tlm-device_id",
        default=[0],
        type=lambda device_ids: [int(x) for x in device_ids.split(",")],
        help="QAIC device ids to run TLM on (comma-separated)",
    )
    argp.add_argument("--prompt-file", type=str, help="txt file with <endofprompt>-separated prompts", dest="prompt_file")
    argp.add_argument("--max-spec-length", type=int, default=7, dest="max_spec_length", help="Length of the speculation generated by the draft LM")
    args = argp.parse_args()
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_name, padding_side="left")
    dlm_session = QAICInferenceSession(f"{args.dlm_qpc}", args.dlm_device_id)
    dlm_session.skip_buffers(set([x for x in dlm_session.output_names if x.endswith("_RetainedState")]))
    tlm_session = QAICInferenceSession(f"{args.tlm_qpc}", args.tlm_device_id)
    tlm_session.skip_buffers(set([x for x in tlm_session.output_names if x.endswith("_RetainedState")]))

    prompt_filetext=""
    with open(args.prompt_file,"r") as f:
        prompt_filetext = f.read()
        #skip the last item since it will be empty
        prompts = [p.strip() for p in prompt_filetext.split("<endofprompt>")][:-1]
    for p in prompts:
        main(tlm_session,dlm_session,tokenizer,args.model_family,\
        args.prompt_len,\
        args.ctx_len,\
        p,\
        args.max_spec_length,\
        args.stream)
